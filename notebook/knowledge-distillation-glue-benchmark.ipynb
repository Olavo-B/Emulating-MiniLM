{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Distillation with BERT on GLUE Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, BertModel\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import the custom distillation loss\n",
    "import sys\n",
    "sys.path.append('/app/src')\n",
    "from loss.distillation import DistillationLoss\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column train not in the dataset. Current columns in the dataset: ['sentence', 'label', 'idx']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m glue_datasets \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m glue_dataset\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m---> 25\u001b[0m     glue_datasets[task] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     26\u001b[0m         split: prepare_data(glue_dataset[task][split])\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     28\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     23\u001b[0m glue_datasets \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m glue_dataset\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m     25\u001b[0m     glue_datasets[task] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 26\u001b[0m         split: prepare_data(\u001b[43mglue_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     28\u001b[0m     }\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:2780\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2779\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:2764\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2762\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m   2763\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2764\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2765\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[1;32m   2766\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[1;32m   2767\u001b[0m )\n\u001b[1;32m   2768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py:590\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    588\u001b[0m         _raise_bad_key_type(key)\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 590\u001b[0m     \u001b[43m_check_valid_column_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    592\u001b[0m     size \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table\u001b[38;5;241m.\u001b[39mnum_rows\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py:527\u001b[0m, in \u001b[0;36m_check_valid_column_key\u001b[0;34m(key, columns)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_valid_column_key\u001b[39m(key: \u001b[38;5;28mstr\u001b[39m, columns: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[0;32m--> 527\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column train not in the dataset. Current columns in the dataset: ['sentence', 'label', 'idx']\""
     ]
    }
   ],
   "source": [
    "# Load GLUE dataset\n",
    "glue_dataset = load_dataset('glue','sst2')\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and prepare data\n",
    "def prepare_data(examples, max_length=128):\n",
    "    encodings = tokenizer(\n",
    "        examples['sentence'], \n",
    "        truncation=True, \n",
    "        padding='max_length', \n",
    "        max_length=max_length\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'input_ids': torch.tensor(encodings['input_ids']),\n",
    "        'attention_mask': torch.tensor(encodings['attention_mask']),\n",
    "        'labels': torch.tensor(examples['label'])\n",
    "    }\n",
    "\n",
    "# Prepare dataset for all GLUE tasks\n",
    "glue_datasets = {}\n",
    "for task in glue_dataset.keys():\n",
    "    glue_datasets[task] = {\n",
    "        split: prepare_data(glue_dataset[task][split])\n",
    "        for split in ['train', 'validation', 'test']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Student Model with Reduced Complexity\n",
    "class StudentBertModel(nn.Module):\n",
    "    def __init__(self, num_labels=2, hidden_size=768, num_hidden_layers=3):\n",
    "        super(StudentBertModel, self).__init__()\n",
    "        \n",
    "        # Base BERT model with reduced layers\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased', \n",
    "                                               num_hidden_layers=num_hidden_layers)\n",
    "        \n",
    "        # Simplified classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size // 2, num_labels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get BERT outputs\n",
    "        bert_outputs = self.bert(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Use the pooled output for classification\n",
    "        pooled_output = bert_outputs.pooler_output\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def get_attention_and_values(self, input_ids, attention_mask):\n",
    "        # Get BERT outputs including attention\n",
    "        bert_outputs = self.bert(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=True\n",
    "        )\n",
    "        \n",
    "        # Return attention and values\n",
    "        return (\n",
    "            bert_outputs.attentions,  # Attention weights\n",
    "            bert_outputs.last_hidden_state  # Value vectors\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Setup\n",
    "batch_size = 32\n",
    "num_epochs = 3\n",
    "learning_rate = 2e-5\n",
    "\n",
    "# Initialize teacher and student models\n",
    "teacher_model = BertForSequenceClassification.from_pretrained('bert-base-uncased').to(device)\n",
    "teacher_base_model = teacher_model.bert  # Extract the base BERT model\n",
    "student_model = StudentBertModel(num_labels=2).to(device)\n",
    "\n",
    "# Freeze teacher model\n",
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Optimizers and loss functions\n",
    "optimizer = torch.optim.AdamW(student_model.parameters(), lr=learning_rate)\n",
    "distillation_criterion = DistillationLoss(temperature=2.0).to(device)\n",
    "classification_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training function\n",
    "def train_model(task):\n",
    "    train_dataset = TensorDataset(\n",
    "        glue_datasets[task]['train']['input_ids'],\n",
    "        glue_datasets[task]['train']['attention_mask'],\n",
    "        glue_datasets[task]['train']['labels']\n",
    "    )\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    student_model.train()\n",
    "    teacher_model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    progress_bar = tqdm(train_dataloader, desc=f'Training {task}', leave=False)\n",
    "    for batch in progress_bar:\n",
    "        # Unpack batch\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass for teacher\n",
    "        with torch.no_grad():\n",
    "            teacher_output = teacher_model(input_ids, attention_mask=attention_mask)\n",
    "            teacher_bert_output = teacher_base_model(\n",
    "                input_ids, \n",
    "                attention_mask=attention_mask, \n",
    "                output_attentions=True\n",
    "            )\n",
    "            teacher_attention = teacher_bert_output.attentions\n",
    "            teacher_values = teacher_bert_output.last_hidden_state\n",
    "        \n",
    "        # Forward pass for student\n",
    "        student_logits = student_model(input_ids, attention_mask)\n",
    "        student_attention, student_values = student_model.get_attention_and_values(input_ids, attention_mask)\n",
    "        \n",
    "        # Compute distillation loss\n",
    "        # Aggregate attention and values across layers\n",
    "        student_A = torch.mean(torch.stack(student_attention), dim=0)\n",
    "        teacher_A = torch.mean(torch.stack(teacher_attention), dim=0)\n",
    "        \n",
    "        knowledge_loss = distillation_criterion(\n",
    "            teacher_A, teacher_values,\n",
    "            student_A, student_values\n",
    "        )\n",
    "        \n",
    "        # Compute classification loss\n",
    "        classification_loss = classification_criterion(student_logits, labels)\n",
    "        \n",
    "        # Combine losses\n",
    "        total_batch_loss = 0.5 * knowledge_loss + 0.5 * classification_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        total_batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += total_batch_loss.item()\n",
    "        progress_bar.set_description(f'Training {task}, Loss: {total_loss/(len(progress_bar)+1):.4f}')\n",
    "\n",
    "    return total_loss / len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on all GLUE tasks\n",
    "for task in glue_dataset.keys():\n",
    "    print(f'Training on {task} task')\n",
    "    train_model(task)\n",
    "    print(f'Finished training on {task} task')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
