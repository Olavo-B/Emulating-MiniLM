{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2ou8MXiWvUe"
      },
      "source": [
        "# Knowledge Distillation with BERT\n",
        "\n",
        "## Overview\n",
        "An advanced implementation of knowledge distillation using a custom distillation loss for sentiment classification."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9vjHIfJXcZA",
        "outputId": "cd2acdd8-a815-4563-a5aa-90bf4ff9f871"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bS5XXMSWvUf",
        "outputId": "27b712ec-3b66-4693-bc86-81d6ff3d2c98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, BertModel\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RUbZNoMWvUg"
      },
      "source": [
        "## Fining Tuning BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ki22uzROWvUg"
      },
      "outputs": [],
      "source": [
        "# Step 1: Import Required Libraries\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# Step 2: Load the SST-2 Dataset\n",
        "dataset = load_dataset('glue', 'sst2')\n",
        "\n",
        "# Step 3: Preprocess the Data\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['sentence'], padding='max_length', truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Step 4: Set up the Model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Step 5: Define the Trainer and Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    eval_strategy=\"epoch\",           # change to eval_strategy\n",
        "    learning_rate=2e-5,              # learning rate\n",
        "    per_device_train_batch_size=16,  # batch size for training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    num_train_epochs=3,              # number of training epochs\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                     # the model to train\n",
        "    args=training_args,              # training arguments\n",
        "    train_dataset=tokenized_datasets['train'],   # training dataset\n",
        "    eval_dataset=tokenized_datasets['validation'], # evaluation dataset\n",
        ")\n",
        "\n",
        "# Step 6: Fine-Tune the Model\n",
        "trainer.train()\n",
        "\n",
        "# Step 7: Evaluate the Model\n",
        "results = trainer.evaluate()\n",
        "\n",
        "# Print evaluation results\n",
        "print(results)\n",
        "\n",
        "# Save the model and tokenizer\n",
        "model.save_pretrained('./fine_tuned_bert')\n",
        "tokenizer.save_pretrained('./fine_tuned_bert')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01-daw7PWvUg",
        "outputId": "bcd4fd31-bc79-4780-b579-fe0a938b13ff"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [28/28 00:08]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.33054065704345703, 'eval_model_preparation_time': 0.0016, 'eval_accuracy': 0.926605504587156, 'eval_runtime': 8.9137, 'eval_samples_per_second': 97.827, 'eval_steps_per_second': 3.141}\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load fine-tuned model and tokenizer\n",
        "model = BertForSequenceClassification.from_pretrained('./fine_tuned_bert')\n",
        "tokenizer = BertTokenizer.from_pretrained('./fine_tuned_bert')\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset('glue', 'sst2')\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['sentence'], padding='max_length', truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Convert to PyTorch format\n",
        "tokenized_datasets = tokenized_datasets.with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # Directory to save results\n",
        "    per_device_eval_batch_size=32,  # Batch size for evaluation\n",
        "    logging_dir='./logs',           # Directory for logs\n",
        "    eval_strategy=\"epoch\",    # Evaluate every epoch\n",
        ")\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.argmax(axis=-1)\n",
        "    return {'accuracy': accuracy_score(labels, predictions)}\n",
        "\n",
        "# Set up Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    eval_dataset=tokenized_datasets['validation'],\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "results = trainer.evaluate()\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import BERT Teacher Model"
      ],
      "metadata": {
        "id": "3gpBl9HWjaJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "Nh16gDNLj4kx",
        "outputId": "d8990da4-2bdf-43f0-bae1-75d09eb59048"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4480b2a3-1e59-4328-9003-d6b4f5ddde53\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4480b2a3-1e59-4328-9003-d6b4f5ddde53\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBDktUL2kBFf",
        "outputId": "b15ceef7-fbcb-4e22-81f8-1c88fd6e7806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pytorch_model.bin  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "# Define the path to your pytorch_model.bin\n",
        "model_path = './pytorch_model.bin'\n",
        "\n",
        "# Load the tokenizer and model using the checkpoint\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "teacher_model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "teacher_model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740,
          "referenced_widgets": [
            "a90ccae05eb04f5284a236aa1787189a",
            "221079c491e24685bdb4fd396932a9f1",
            "322abb8a1731491f837a28b01b6895ec",
            "b580c68d60ed418fba50ea70b263c2aa",
            "cf37b3f5caeb45ab8d75069be8709448",
            "aa4ecaf6531941cb8fddd535f16cc18d",
            "ffe3fdec376f428bbfda376a3d44f8bd",
            "f484a3d5c9944db4a764560d297edf2a",
            "dd2d0fd2c4544efbacfa092f6f5f5a24",
            "612c8863cfb742f097ee9127890ccf2c",
            "1e19d2d829c44d1493a7d4f49ec1b7b3",
            "3b2dedd0619f40cbb9ff476e4bf31abd",
            "a4d3de7143cd4fb48755f5f234ef9829",
            "1e10f25140a84211a737e14d78cd9935",
            "f28aac86b803435a8868523c515cf137",
            "ea3e652263354941926e9820bc36d16e",
            "a916611321ee46fb8ec725f033047dc1",
            "8b33ffb6234f4fefb1cd0bb499adc0db",
            "f4a9ba4237724b7d99de9fb14058b734",
            "80d8709415b14a6dac376d0303b98fa4",
            "95246d4f74f44577ac6cbbea1e86f271",
            "22f60c58b5484c059811b7d4660af41b",
            "00cde1671cd44a118efec93b56bada31",
            "fbf415227bf9464f96fb76cb13eb595c",
            "ce9f20ade8364ef49f52e1a2c6b67126",
            "b1ba040556ef4d94bb984886cca2cc49",
            "cb6f620d36ba4297b64459379cc9f018",
            "23a7164bab094c128b1346af4e278243",
            "222cc62be50d4a0a98f8a8150fe34cb6",
            "bff6b5d36dae4a28a97a08a03fc68012",
            "91fabf57e0864c2e877131b96ee5c5ce",
            "c78bc3b70fd9475bb2bc5e390314526e",
            "83090fd429a14b0790977e261658a804",
            "dd779362b072466897e77485714738f4",
            "439c90272730435ea063b2b6a7bfbb5e",
            "66494460a73144c78e86dd043b93edc3",
            "2a5b8065accc4ba08d88c923bc9b5bbc",
            "9bf2b6977370492ca4ec48afbf570c44",
            "1e5556a01ff44725aa3c655efcf555e0",
            "0ed737d0148a4875b78d1f10fd62f3b7",
            "197f3b03756d4f9db87426b0996d0c9f",
            "ade14c7da6304a058d87e9ba83f7e240",
            "cb218fd6163c4816b1f6fb5abc38dee2",
            "c40d654079e547f787fb27c89a3f0825"
          ]
        },
        "id": "NlYccsA6jeOO",
        "outputId": "e73a6c2c-70fe-44f0-bb47-5f564dd9f1a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a90ccae05eb04f5284a236aa1787189a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b2dedd0619f40cbb9ff476e4bf31abd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00cde1671cd44a118efec93b56bada31"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd779362b072466897e77485714738f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Incorrect path_or_model_id: './pytorch_model.bin'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mREPO_ID_REGEX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;34m\"Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './pytorch_model.bin'.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-6d77d6f0b07d>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the tokenizer and model using the checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mteacher_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Ensure the model is in evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3505\u001b[0m                 \u001b[0;31m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3506\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m   3507\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3508\u001b[0m                     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"There was a specific connection error when trying to load {path_or_repo_id}:\\n{err}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHFValidationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    470\u001b[0m             \u001b[0;34mf\"Incorrect path_or_model_id: '{path_or_repo_id}'. Please provide either the path to a local folder or the repo_id of a model on the Hub.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         ) from e\n",
            "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: './pytorch_model.bin'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## My Distillation Class"
      ],
      "metadata": {
        "id": "d0jyU0y8l6vy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DistillationLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the distillation loss for MINILM as described in the paper.\n",
        "\n",
        "    This loss consists of two components:\n",
        "    1. Attention Distribution Transfer Loss\n",
        "    2. Value-Relation Transfer Loss\n",
        "\n",
        "    The loss captures the knowledge transfer between teacher and student models\n",
        "    by minimizing the KL-divergence of attention distributions and value relations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, temperature=2.0):\n",
        "        \"\"\"\n",
        "        Initializes the DistillationLoss module.\n",
        "\n",
        "        Args:\n",
        "            temperature (float): Softening temperature for the softmax distributions.\n",
        "                               Default is 2.0 as suggested in the paper.\n",
        "        \"\"\"\n",
        "        super(DistillationLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def _compute_attention_distribution(self, attention_scores):\n",
        "        \"\"\"\n",
        "        Computes the attention distribution using scaled dot-product.\n",
        "\n",
        "        Args:\n",
        "            queries (torch.Tensor): Query vectors\n",
        "            keys (torch.Tensor): Key vectors\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Attention distribution\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        # Apply softmax with temperature scaling\n",
        "        attention_dist = F.softmax(attention_scores / self.temperature, dim=-1)\n",
        "        return attention_dist\n",
        "\n",
        "    def _compute_value_relation(self, values):\n",
        "        \"\"\"\n",
        "        Computes the value relation matrix using scaled dot-product.\n",
        "\n",
        "        Args:\n",
        "            values (torch.Tensor): Value vectors\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Value relation matrix\n",
        "        \"\"\"\n",
        "        # Compute scaled dot-product between values\n",
        "        value_relation = torch.matmul(values, values.transpose(-2, -1))\n",
        "        value_relation = value_relation / (values.size(-1) ** 0.5)\n",
        "\n",
        "        # Apply softmax\n",
        "        value_relation_dist = F.softmax(value_relation, dim=-1)\n",
        "        return value_relation_dist\n",
        "\n",
        "    def forward(self,\n",
        "            teacher_A, teacher_values,\n",
        "            student_A, student_values):\n",
        "        \"\"\"\n",
        "        Computes the distillation loss between teacher and student models\n",
        "        with proper LAT (attention transfer) and VR (value relation) scaling.\n",
        "\n",
        "        Args:\n",
        "            teacher_A (torch.Tensor): Attention distribution from the teacher model\n",
        "            teacher_values (torch.Tensor): Value vectors from the teacher model\n",
        "            student_A (torch.Tensor): Attention distribution from the student model\n",
        "            student_values (torch.Tensor): Value vectors from the student model\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Total distillation loss with LAT and VR scaling.\n",
        "        \"\"\"\n",
        "\n",
        "        # Ensure all tensors require gradients\n",
        "        # Check gradients of relevant tensors\n",
        "\n",
        "        # Ensure all tensors involved in loss computation are part of the computation graph\n",
        "\n",
        "\n",
        "        student_A = self._compute_attention_distribution(student_A)\n",
        "        teacher_A = self._compute_attention_distribution(teacher_A)\n",
        "\n",
        "\n",
        "        # Attention Distribution Transfer (LAT) - KL Divergence\n",
        "        kl_attention = F.kl_div(\n",
        "            torch.log(student_A + 1e-8),  # Log with stability added\n",
        "            teacher_A, reduction=\"none\"\n",
        "        )  # Shape: [batch, A_h, |x|, |x|]\n",
        "\n",
        "        Ah = student_A.size(1)  # Number of attention heads\n",
        "        X = student_A.size(2)   # Sequence length\n",
        "\n",
        "        # Summing over sequence positions and averaging over attention heads\n",
        "        attention_transfer_loss = kl_attention.sum(dim=(-2, -1)).mean() / (Ah * X)\n",
        "\n",
        "        # Value Relation Transfer (VR) - KL Divergence for value relations\n",
        "        teacher_value_relation = self._compute_value_relation(teacher_values)\n",
        "        student_value_relation = self._compute_value_relation(student_values)\n",
        "\n",
        "        kl_value_relation = F.kl_div(\n",
        "            torch.log(student_value_relation + 1e-8),  # Log with stability added\n",
        "            teacher_value_relation, reduction=\"none\"\n",
        "        )  # Shape: [batch, A_h, |x|, |x|]\n",
        "\n",
        "        # Summing over sequence positions and averaging over attention heads\n",
        "        value_relation_loss = kl_value_relation.sum(dim=(-2, -1)).mean() / (Ah * X)\n",
        "\n",
        "        # Total loss as the sum of both components\n",
        "        total_loss = attention_transfer_loss + value_relation_loss\n",
        "\n",
        "        # Ensure total loss requires gradients\n",
        "        assert total_loss.requires_grad, \"Total loss must require gradients!\"\n",
        "\n",
        "        return total_loss\n"
      ],
      "metadata": {
        "id": "RK4CHIlTmA_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXTccMRKWvUh"
      },
      "source": [
        "## Knowledge Distillation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TE7OgnXCWvUh"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "dataset = load_dataset('glue', 'sst2')['train']\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize and prepare data\n",
        "def prepare_data(examples, max_length=512):\n",
        "    encodings = tokenizer(\n",
        "        examples['sentence'],\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=max_length\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'input_ids': torch.tensor(encodings['input_ids']),\n",
        "        'attention_mask': torch.tensor(encodings['attention_mask']),\n",
        "        'labels': torch.tensor(examples['label'])\n",
        "    }\n",
        "\n",
        "# Prepare dataset\n",
        "processed_data = prepare_data(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxY8sccKWvUi"
      },
      "outputs": [],
      "source": [
        "# Custom Student Model with Reduced Complexity\n",
        "class StudentBertModel(nn.Module):\n",
        "    def __init__(self, num_labels=2, hidden_size=768, num_hidden_layers=6):\n",
        "        super(StudentBertModel, self).__init__()\n",
        "\n",
        "        # Base BERT model with reduced layers\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased',\n",
        "                                               num_hidden_layers=num_hidden_layers,\n",
        "                                               attn_implementation=\"eager\")\n",
        "\n",
        "        # Simplified classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_size // 2, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Get BERT outputs\n",
        "        bert_outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        # Use the pooled output for classification\n",
        "        pooled_output = bert_outputs.pooler_output\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def get_attention_and_values(self, input_ids, attention_mask):\n",
        "        # Get BERT outputs including attention\n",
        "        bert_outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_attentions=True\n",
        "        )\n",
        "\n",
        "        # Return attention and values\n",
        "        return (\n",
        "            bert_outputs.attentions,  # Attention weights\n",
        "            bert_outputs.last_hidden_state  # Value vectors\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spQmf8R_WvUi"
      },
      "outputs": [],
      "source": [
        "# Training Hyperparameters\n",
        "batch_size = 32\n",
        "num_epochs = 100\n",
        "learning_rate = 2e-5\n",
        "\n",
        "# Create DataLoader\n",
        "dataset = TensorDataset(\n",
        "    processed_data['input_ids'],\n",
        "    processed_data['attention_mask'],\n",
        "    processed_data['labels']\n",
        ")\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Initialize teacher and student models\n",
        "teacher_model = model\n",
        "\n",
        "teacher_base_model = teacher_model.bert  # Extract the base BERT model\n",
        "student_model = StudentBertModel(num_labels=2).to(device)\n",
        "\n",
        "# Freeze teacher model\n",
        "for param in teacher_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Training Setup\n",
        "optimizer = torch.optim.AdamW(student_model.parameters(), lr=2e-5)\n",
        "distillation_criterion = DistillationLoss(temperature=2.0).to(device)\n",
        "classification_criterion = nn.CrossEntropyLoss()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkDALY3PWvUi",
        "outputId": "a3986d62-baae-439c-aca0-3662e81abd69"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                          \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Training Loss: 0.0140\n",
            "Epoch 1, Validation Accuracy: 0.9130\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                          \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2, Training Loss: 0.0138\n",
            "Epoch 2, Validation Accuracy: 0.9090\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                          \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3, Training Loss: 0.0135\n",
            "Epoch 3, Validation Accuracy: 0.9090\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                          \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4, Training Loss: 0.0133\n",
            "Epoch 4, Validation Accuracy: 0.9090\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                          \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5, Training Loss: 0.0131\n",
            "Epoch 5, Validation Accuracy: 0.9090\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                          \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6, Training Loss: 0.0127\n",
            "Epoch 6, Validation Accuracy: 0.9090\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                          \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7, Training Loss: 0.0126\n",
            "Epoch 7, Validation Accuracy: 0.9100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                          \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8, Training Loss: 0.0125\n",
            "Epoch 8, Validation Accuracy: 0.9100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                          \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9, Training Loss: 0.0122\n",
            "Epoch 9, Validation Accuracy: 0.9110\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10, Training Loss: 0.0122\n",
            "Epoch 10, Validation Accuracy: 0.9100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11, Training Loss: 0.0117\n",
            "Epoch 11, Validation Accuracy: 0.9130\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12, Training Loss: 0.0118\n",
            "Epoch 12, Validation Accuracy: 0.9130\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13, Training Loss: 0.0114\n",
            "Epoch 13, Validation Accuracy: 0.9130\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14, Training Loss: 0.0115\n",
            "Epoch 14, Validation Accuracy: 0.9130\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15, Training Loss: 0.0113\n",
            "Epoch 15, Validation Accuracy: 0.9140\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16, Training Loss: 0.0112\n",
            "Epoch 16, Validation Accuracy: 0.9150\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17, Training Loss: 0.0110\n",
            "Epoch 17, Validation Accuracy: 0.9160\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18, Training Loss: 0.0109\n",
            "Epoch 18, Validation Accuracy: 0.9160\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19, Training Loss: 0.0107\n",
            "Epoch 19, Validation Accuracy: 0.9160\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20, Training Loss: 0.0107\n",
            "Epoch 20, Validation Accuracy: 0.9150\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21, Training Loss: 0.0103\n",
            "Epoch 21, Validation Accuracy: 0.9130\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22, Training Loss: 0.0103\n",
            "Epoch 22, Validation Accuracy: 0.9110\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23, Training Loss: 0.0102\n",
            "Epoch 23, Validation Accuracy: 0.9110\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24, Training Loss: 0.0103\n",
            "Epoch 24, Validation Accuracy: 0.9110\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25, Training Loss: 0.0100\n",
            "Epoch 25, Validation Accuracy: 0.9100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26, Training Loss: 0.0100\n",
            "Epoch 26, Validation Accuracy: 0.9080\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27, Training Loss: 0.0098\n",
            "Epoch 27, Validation Accuracy: 0.9080\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28, Training Loss: 0.0095\n",
            "Epoch 28, Validation Accuracy: 0.9050\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29, Training Loss: 0.0095\n",
            "Epoch 29, Validation Accuracy: 0.9020\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30, Training Loss: 0.0095\n",
            "Epoch 30, Validation Accuracy: 0.9020\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 31, Training Loss: 0.0092\n",
            "Epoch 31, Validation Accuracy: 0.9010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 32, Training Loss: 0.0092\n",
            "Epoch 32, Validation Accuracy: 0.8980\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 33, Training Loss: 0.0090\n",
            "Epoch 33, Validation Accuracy: 0.8960\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 34, Training Loss: 0.0090\n",
            "Epoch 34, Validation Accuracy: 0.8940\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 35, Training Loss: 0.0090\n",
            "Epoch 35, Validation Accuracy: 0.8920\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 36, Training Loss: 0.0090\n",
            "Epoch 36, Validation Accuracy: 0.8910\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 37, Training Loss: 0.0086\n",
            "Epoch 37, Validation Accuracy: 0.8910\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 38, Training Loss: 0.0086\n",
            "Epoch 38, Validation Accuracy: 0.8890\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 39, Training Loss: 0.0087\n",
            "Epoch 39, Validation Accuracy: 0.8880\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 40, Training Loss: 0.0085\n",
            "Epoch 40, Validation Accuracy: 0.8900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 41, Training Loss: 0.0085\n",
            "Epoch 41, Validation Accuracy: 0.8880\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 42, Training Loss: 0.0084\n",
            "Epoch 42, Validation Accuracy: 0.8880\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 43, Training Loss: 0.0082\n",
            "Epoch 43, Validation Accuracy: 0.8880\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 44, Training Loss: 0.0081\n",
            "Epoch 44, Validation Accuracy: 0.8880\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 45, Training Loss: 0.0081\n",
            "Epoch 45, Validation Accuracy: 0.8910\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 46, Training Loss: 0.0080\n",
            "Epoch 46, Validation Accuracy: 0.8950\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 47, Training Loss: 0.0079\n",
            "Epoch 47, Validation Accuracy: 0.8940\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 48, Training Loss: 0.0079\n",
            "Epoch 48, Validation Accuracy: 0.8970\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 49, Training Loss: 0.0079\n",
            "Epoch 49, Validation Accuracy: 0.8960\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50, Training Loss: 0.0078\n",
            "Epoch 50, Validation Accuracy: 0.9040\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 51, Training Loss: 0.0076\n",
            "Epoch 51, Validation Accuracy: 0.9010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 52, Training Loss: 0.0075\n",
            "Epoch 52, Validation Accuracy: 0.9030\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 53, Training Loss: 0.0075\n",
            "Epoch 53, Validation Accuracy: 0.9000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 54, Training Loss: 0.0076\n",
            "Epoch 54, Validation Accuracy: 0.9010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 55, Training Loss: 0.0074\n",
            "Epoch 55, Validation Accuracy: 0.9020\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 56, Training Loss: 0.0072\n",
            "Epoch 56, Validation Accuracy: 0.9010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 57, Training Loss: 0.0072\n",
            "Epoch 57, Validation Accuracy: 0.9030\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 58, Training Loss: 0.0072\n",
            "Epoch 58, Validation Accuracy: 0.9050\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 59, Training Loss: 0.0072\n",
            "Epoch 59, Validation Accuracy: 0.9040\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 60, Training Loss: 0.0072\n",
            "Epoch 60, Validation Accuracy: 0.9010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 61, Training Loss: 0.0069\n",
            "Epoch 61, Validation Accuracy: 0.8990\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 62, Training Loss: 0.0068\n",
            "Epoch 62, Validation Accuracy: 0.8990\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 63, Training Loss: 0.0066\n",
            "Epoch 63, Validation Accuracy: 0.8980\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 64, Training Loss: 0.0067\n",
            "Epoch 64, Validation Accuracy: 0.8970\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 65, Training Loss: 0.0066\n",
            "Epoch 65, Validation Accuracy: 0.8980\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 66, Training Loss: 0.0065\n",
            "Epoch 66, Validation Accuracy: 0.8990\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 67, Training Loss: 0.0064\n",
            "Epoch 67, Validation Accuracy: 0.8990\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 68, Training Loss: 0.0063\n",
            "Epoch 68, Validation Accuracy: 0.8950\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 69, Training Loss: 0.0063\n",
            "Epoch 69, Validation Accuracy: 0.8950\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 70, Training Loss: 0.0062\n",
            "Epoch 70, Validation Accuracy: 0.8950\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 71, Training Loss: 0.0062\n",
            "Epoch 71, Validation Accuracy: 0.8950\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 72, Training Loss: 0.0061\n",
            "Epoch 72, Validation Accuracy: 0.8920\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 73, Training Loss: 0.0061\n",
            "Epoch 73, Validation Accuracy: 0.8920\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 74, Training Loss: 0.0058\n",
            "Epoch 74, Validation Accuracy: 0.8890\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 75, Training Loss: 0.0058\n",
            "Epoch 75, Validation Accuracy: 0.8860\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 76, Training Loss: 0.0059\n",
            "Epoch 76, Validation Accuracy: 0.8860\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 77, Training Loss: 0.0058\n",
            "Epoch 77, Validation Accuracy: 0.8830\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 78, Training Loss: 0.0057\n",
            "Epoch 78, Validation Accuracy: 0.8800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 79, Training Loss: 0.0055\n",
            "Epoch 79, Validation Accuracy: 0.8760\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 80, Training Loss: 0.0055\n",
            "Epoch 80, Validation Accuracy: 0.8750\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 81, Training Loss: 0.0055\n",
            "Epoch 81, Validation Accuracy: 0.8700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 82, Training Loss: 0.0053\n",
            "Epoch 82, Validation Accuracy: 0.8670\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 83, Training Loss: 0.0053\n",
            "Epoch 83, Validation Accuracy: 0.8660\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 84, Training Loss: 0.0052\n",
            "Epoch 84, Validation Accuracy: 0.8640\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 85, Training Loss: 0.0052\n",
            "Epoch 85, Validation Accuracy: 0.8640\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 86, Training Loss: 0.0051\n",
            "Epoch 86, Validation Accuracy: 0.8620\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 87, Training Loss: 0.0051\n",
            "Epoch 87, Validation Accuracy: 0.8600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 88, Training Loss: 0.0050\n",
            "Epoch 88, Validation Accuracy: 0.8570\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 89, Training Loss: 0.0049\n",
            "Epoch 89, Validation Accuracy: 0.8540\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 90, Training Loss: 0.0049\n",
            "Epoch 90, Validation Accuracy: 0.8530\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 91, Training Loss: 0.0048\n",
            "Epoch 91, Validation Accuracy: 0.8480\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 92, Training Loss: 0.0048\n",
            "Epoch 92, Validation Accuracy: 0.8470\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 93, Training Loss: 0.0047\n",
            "Epoch 93, Validation Accuracy: 0.8470\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 94, Training Loss: 0.0046\n",
            "Epoch 94, Validation Accuracy: 0.8510\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 95, Training Loss: 0.0046\n",
            "Epoch 95, Validation Accuracy: 0.8470\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 96, Training Loss: 0.0046\n",
            "Epoch 96, Validation Accuracy: 0.8480\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 97, Training Loss: 0.0045\n",
            "Epoch 97, Validation Accuracy: 0.8480\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 98, Training Loss: 0.0044\n",
            "Epoch 98, Validation Accuracy: 0.8470\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 99, Training Loss: 0.0043\n",
            "Epoch 99, Validation Accuracy: 0.8480\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100, Training Loss: 0.0043\n",
            "Epoch 100, Validation Accuracy: 0.8520\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize TensorBoard writer\n",
        "writer = SummaryWriter(log_dir=\"runs/knowledge_distillation\")\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "    student_model.train()\n",
        "    teacher_model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        # Unpack batch\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass for teacher\n",
        "        with torch.no_grad():\n",
        "            teacher_output = teacher_model(input_ids, attention_mask=attention_mask)\n",
        "            teacher_bert_output = teacher_base_model(\n",
        "                input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                output_attentions=True\n",
        "            )\n",
        "            teacher_attention = teacher_bert_output.attentions\n",
        "            teacher_values = teacher_bert_output.last_hidden_state\n",
        "\n",
        "        # Forward pass for student\n",
        "        student_logits = student_model(input_ids, attention_mask)\n",
        "        student_attention, student_values = student_model.get_attention_and_values(input_ids, attention_mask)\n",
        "\n",
        "        # Compute distillation loss\n",
        "        student_A = student_attention[-1]\n",
        "        teacher_A = teacher_attention[-1]\n",
        "\n",
        "        knowledge_loss = distillation_criterion(\n",
        "            teacher_A, teacher_values,\n",
        "            student_A, student_values\n",
        "        )\n",
        "\n",
        "        # Compute classification loss\n",
        "        classification_loss = classification_criterion(student_logits, labels)\n",
        "\n",
        "        # Combine losses\n",
        "        total_batch_loss = knowledge_loss\n",
        "\n",
        "        # Backward pass\n",
        "        total_batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += total_batch_loss.item()\n",
        "\n",
        "        # Log batch loss to TensorBoard\n",
        "        writer.add_scalar(\"Loss/Batch\", total_batch_loss.item(), epoch * len(dataloader) + batch_idx)\n",
        "\n",
        "        progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_batch_loss.item():.4f}\")\n",
        "\n",
        "    # Compute and log average training loss\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    writer.add_scalar(\"Loss/Epoch\", avg_loss, epoch)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Training Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    student_model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "            logits = student_model(input_ids, attention_mask)\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "            correct += torch.sum(predictions == labels).item()\n",
        "            total += len(labels)\n",
        "\n",
        "    # Compute and log validation accuracy\n",
        "    accuracy = correct / total\n",
        "    writer.add_scalar(\"Accuracy/Validation\", accuracy, epoch)\n",
        "    print(f\"Epoch {epoch+1}, Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Close the TensorBoard writer\n",
        "writer.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlU7p42WWvUj"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Save student model\n",
        "torch.save(student_model.state_dict(), './runs/student_model.pt')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}